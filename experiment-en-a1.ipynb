{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction in Condmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-08-10T08:51:58.719167Z",
     "iopub.status.idle": "2020-08-10T08:51:58.719413Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import math\n",
    "from typing import List, Any, Dict, Tuple\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV, train_test_split\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Typing\n",
    "NodePair = Tuple[int, int]\n",
    "Edge = List[Tuple[int, int, Dict[str, datetime]]]\n",
    "\n",
    "folder = '/local/bruingjde/complexnetworks2020-experiment/temp/en-a1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T08:31:26.355338Z",
     "iopub.status.busy": "2020-08-10T08:31:26.355177Z",
     "iopub.status.idle": "2020-08-10T08:31:26.369779Z",
     "shell.execute_reply": "2020-08-10T08:31:26.369147Z",
     "shell.execute_reply.started": "2020-08-10T08:31:26.355320Z"
    }
   },
   "outputs": [],
   "source": [
    "def _filter_edgelist(edges: List[Edge], start, stop) -> List[Edge]: \n",
    "  \"\"\"Filter edgelist.  If start/ stop is float, start/stop from the fraction of total edges. If datetime, this is used.\"\"\" \n",
    "  no_edges = len(edges)\n",
    "  if start is None: start=0\n",
    "  if stop is None: stop=1\n",
    "  if type(start) is float or start == 0:\n",
    "    start_index = int(start*no_edges)\n",
    "    start = edges[start_index][2]['date']\n",
    "  if type(stop) is float or stop == 1:\n",
    "    stop_index = math.floor(stop*no_edges)-1\n",
    "    stop = edges[stop_index][2]['date']\n",
    "  return [edge for edge in edges if edge[2]['date'] >= start and edge[2]['date'] <= stop]\n",
    "def get_edgelist(file='src/enron.pkl', start=None, stop=None) -> List[Edge]:\n",
    "  return _filter_edgelist(joblib.load(file), start, stop)\n",
    "def giant_component(graph: nx.Graph) -> nx.Graph: return graph.subgraph(max(nx.connected_components(graph), key=len)).copy()\n",
    "def get_graph(edgelist: List[Edge]) -> nx.Graph:\n",
    "  \"\"\"Add edge to graph. Contains edge attribute weight.\"\"\"\n",
    "  g = nx.Graph()\n",
    "  \n",
    "  for u, v, _ in edgelist:\n",
    "    weight = g[u][v][\"weight\"]+1 if g.has_edge(u,v) else 1\n",
    "    g.add_edge(u, v, weight=weight)\n",
    "  \n",
    "  return g\n",
    "def report(graph:nx.Graph, probes: Tuple[Author, Author]):\n",
    "  n = len(probes)\n",
    "  print(f\"Number of probes: {n}\")\n",
    "  a = sum([graph.has_edge(u, v) for u, v in probes])\n",
    "  print(f\"- already edge: {a} ({a/n:.0%})\")\n",
    "  non_edges = set(nx.non_edges(graph))\n",
    "  ne = sum([np in non_edges for np in tqdm(probes)])\n",
    "  print(f\"- both nodes in graph: {ne} ({ne/n:.0%})\")\n",
    "  ng = sum([not (graph.has_node(u) and graph.has_node(v)) for u, v in tqdm(probes)])\n",
    "  print(f\"- not in graph: {ng} ({ng/n:.0%})\")\n",
    "def get_distances(graph: nx.Graph, cutoff: int = None) -> (List[NodePair], List[int]):\n",
    "  \"\"\"\n",
    "  Get all non-edges using BFS. When cutoff provided, consider only node pairs with at most this distance.\n",
    "  Returns:\n",
    "  - nodepairs: tuple containing all nodepairs\n",
    "  - distances: tuple containing all distances\n",
    "  \"\"\"\n",
    "  return zip(\n",
    "    *[\n",
    "      [(u, v), distance]\n",
    "      for u, (nbs_u, _) in tqdm(nx.all_pairs_dijkstra(graph, cutoff, weight=None), total=len(graph), desc=\"get_distances\")\n",
    "      for v, distance in nbs_u.items() if distance > 1 and (cutoff is None or distance <= cutoff) \n",
    "    ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "Choose here the parameters on how you want to define the learn and assessing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T08:31:43.322171Z",
     "iopub.status.busy": "2020-08-10T08:31:43.321969Z",
     "iopub.status.idle": "2020-08-10T08:32:29.222299Z",
     "shell.execute_reply": "2020-08-10T08:32:29.221391Z",
     "shell.execute_reply.started": "2020-08-10T08:31:43.322149Z"
    }
   },
   "outputs": [],
   "source": [
    "g_learn = giant_component(get_graph(get_edgelist(stop=.7)))\n",
    "uv_assessing = {(u, v) for u, v, _ in get_edgelist(start=.7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T08:54:59.935966Z",
     "iopub.status.busy": "2020-08-10T08:54:59.935763Z",
     "iopub.status.idle": "2020-08-10T08:55:06.179097Z",
     "shell.execute_reply": "2020-08-10T08:55:06.178639Z",
     "shell.execute_reply.started": "2020-08-10T08:54:59.935946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/local/bruingjde/complexnetworks2020-experiment/temp/en-a1//probes.pkl']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(g_learn, f'{folder}/graph.pkl')\n",
    "joblib.dump(uv_assessing, f'{folder}/probes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(graph=g_learn, probes=uv_assessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-05T07:25:12.679864Z",
     "iopub.status.busy": "2020-08-05T07:25:12.679699Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "targets = joblib.load(f'{folder}all/targets.pkl')\n",
    "nodepairs = joblib.load(f'{folder}all/nodepairs.pkl')\n",
    "distances = joblib.load(f'{folder}all/distances.pkl')\n",
    "g_learn = joblib.load(f'{folder}all/graph.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T09:20:00.488793Z",
     "iopub.status.busy": "2020-08-10T09:20:00.488424Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_distances:   6%|â–Œ         | 3561/60313 [12:16<2:56:43,  5.35it/s] "
     ]
    }
   ],
   "source": [
    "nodepairs, distances = get_distances(g_learn, cutoff=3)\n",
    "targets = [nodepair in uv_assessing for nodepair in tqdm(nodepairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T09:10:00.297963Z",
     "iopub.status.busy": "2020-08-10T09:10:00.297760Z",
     "iopub.status.idle": "2020-08-10T09:10:45.704228Z",
     "shell.execute_reply": "2020-08-10T09:10:45.703250Z",
     "shell.execute_reply.started": "2020-08-10T09:10:00.297944Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nodepairs = np.array(nodepairs)\n",
    "distances = np.array(distances)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-05T07:46:03.487726Z",
     "iopub.status.busy": "2020-08-05T07:46:03.487424Z",
     "iopub.status.idle": "2020-08-05T07:46:08.230196Z",
     "shell.execute_reply": "2020-08-05T07:46:08.229427Z",
     "shell.execute_reply.started": "2020-08-05T07:46:03.487687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for select_distance in [2, 3, 4]:\n",
    "  print(select_distance)\n",
    "  filter_indices = (distances == select_distance)\n",
    "  nodepairs[filter_indices].dump(f'{folder}{select_distance}/nodepairs.pkl')\n",
    "  distances[filter_indices].dump(f'{folder}{select_distance}/distances.pkl')\n",
    "  targets[filter_indices].dump(f'{folder}{select_distance}/targets.pkl')\n",
    "  joblib.dump(g_learn, f'{folder}{select_distance}/graph.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-04T19:43:31.305085Z",
     "iopub.status.busy": "2020-08-04T19:43:31.304897Z",
     "iopub.status.idle": "2020-08-04T19:43:34.251050Z",
     "shell.execute_reply": "2020-08-04T19:43:34.250087Z",
     "shell.execute_reply.started": "2020-08-04T19:43:31.305064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.935012e-05\n"
     ]
    }
   ],
   "source": [
    "print(f'{sum(targets) / len(nodepairs):e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(distances=distances, targets=targets))\n",
    "df = df.groupby('distances')['targets'].agg(['size', 'sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['size'], fill='tozeroy', name='# Nodepairs'))\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['sum'], fill='tozeroy', name='# Positives'))\n",
    "fig.update_layout(xaxis=dict(tickmode='linear', tick0=2, dtick=1), yaxis_type=\"log\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['size'].cumsum(), fill='tozeroy', name='# Nodepairs'))\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['sum'].cumsum(), fill='tozeroy', name='# Positives'))\n",
    "fig.update_layout(xaxis=dict(tickmode='linear', tick0=2, dtick=1), yaxis_type=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = joblib.load(\"temp/a4n/2/features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(df.corr(), x=df.columns, y=df.columns)\n",
    "fig.update_xaxes(side=\"top\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot(df):\n",
    "  return sns.pairplot(\n",
    "    df[df['target']].append(df[~df['target']].sample(sum(df['target']))).apply(minmax_scale), \n",
    "    hue='target',\n",
    "    hue_order=[True, False],\n",
    "    palette={True: 'green', False: 'red'},\n",
    "    kind='reg',\n",
    "    diag_kws=dict(bw=.02),\n",
    "    plot_kws=dict(scatter_kws=dict(alpha=.1))\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `parameter_optimalization.ipynb`. We choose the following parameters:\n",
    "- `max_depth = 1`\n",
    "- `tree_method = 'hist'`\n",
    "- `no feature scaling`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(df: pd.DataFrame, random_state=1, also_random=True, max_depth=[1, 2]) -> pd.DataFrame:\n",
    "  X = df.drop(columns='target').values\n",
    "  y = df['target'].values\n",
    "  \n",
    "  param_grid=dict(max_depth=max_depth, scale_pos_weight=[sum(~y)/sum(y), 1])\n",
    "  \n",
    "  X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=1/3, random_state=random_state)\n",
    "  clf = XGBClassifier(random_state=random_state, tree_method='hist', n_jobs=6)\n",
    "  gridsearch = GridSearchCV(\n",
    "    clf, \n",
    "    param_grid=param_grid, \n",
    "    scoring='average_precision', \n",
    "    n_jobs=30,\n",
    "    cv=StratifiedKFold(shuffle=True, random_state=random_state),\n",
    "    return_train_score=True\n",
    "  )\n",
    "  \n",
    "  if also_random: \n",
    "    gridsearch_random = copy.deepcopy(gridsearch)\n",
    "    np.random.seed(random_state)\n",
    "    y_random = copy.deepcopy(y_trainval)\n",
    "    np.random.shuffle(y_random)\n",
    "  \n",
    "  gridsearch.fit(X_trainval, y_trainval)\n",
    "  df_dict = dict(\n",
    "      mean_train=gridsearch.cv_results_['mean_train_score'],\n",
    "      std_train=gridsearch.cv_results_['std_train_score'],\n",
    "      mean_test=gridsearch.cv_results_['mean_test_score'],\n",
    "      std_test=gridsearch.cv_results_['std_test_score'],\n",
    "      test_fold0=gridsearch.cv_results_[f'split0_test_score'],\n",
    "      test_fold1=gridsearch.cv_results_[f'split1_test_score'],\n",
    "      test_fold2=gridsearch.cv_results_[f'split2_test_score'],\n",
    "      test_fold3=gridsearch.cv_results_[f'split3_test_score'],\n",
    "      test_fold4=gridsearch.cv_results_[f'split4_test_score']\n",
    "  )\n",
    "  \n",
    "  if also_random: \n",
    "    gridsearch_random.fit(X_trainval, y_random)\n",
    "    df_dict['mean_train_random']=gridsearch_random.cv_results_['mean_train_score']\n",
    "    df_dict['std_train_random']=gridsearch_random.cv_results_['std_train_score']\n",
    "    df_dict['mean_test_random']=gridsearch_random.cv_results_['mean_test_score']\n",
    "    df_dict['std_test_random']=gridsearch_random.cv_results_['std_test_score']\n",
    "  df = pd.DataFrame(df_dict, index=pd.Index([(d['max_depth'], d['scale_pos_weight'] > 1) for d in gridsearch.cv_results_['params']], name=('max_depth', 'balanced')))\n",
    "  df['diff_train_test'] = (df['mean_test'] - df['mean_train']).abs()\n",
    "  df['rstd_test'] = df['std_test'] / df['mean_test']\n",
    "  if also_random: df['test_over_random'] = df['mean_test'] - df['mean_test_random']\n",
    "  return df.sort_values('mean_test', ascending=False)\n",
    "    \n",
    "def report_performance(df: pd.DataFrame, random_state=1, max_depth=1, tree_method='hist', balanced=True, n_jobs=128):\n",
    "  X = df.drop(columns='target').values\n",
    "  y = df['target'].values\n",
    "  X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=1/3, random_state=random_state)\n",
    "  clf = XGBClassifier(max_depth=max_depth, n_jobs=128, tree_method=tree_method, scale_pos_weight=sum(~y)/sum(y) if balanced else 1 , random_state=random_state)\n",
    "  clf.fit(X_trainval, y_trainval)\n",
    "  y_pred = clf.predict_proba(X_test)[:,1]\n",
    "  return average_precision_score(y_test, y_pred), roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f'temp/a1/2/features.pkl')\n",
    "X = df.drop(columns='target').values\n",
    "y = df['target'].values\n",
    "\n",
    "param_grid=dict(max_depth=[1, 2], scale_pos_weight=[sum(~y)/sum(y), 1])\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=1/3, random_state=1)\n",
    "clf = XGBClassifier(random_state=1, tree_method='hist', n_jobs=6)\n",
    "gridsearch = GridSearchCV(\n",
    "  clf, \n",
    "  param_grid=param_grid, \n",
    "  scoring='average_precision', \n",
    "  n_jobs=30,\n",
    "  cv=StratifiedKFold(shuffle=True, random_state=1),\n",
    "  return_train_score=True\n",
    ")\n",
    "gridsearch.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps2 = gridsearch(pd.read_pickle(f'temp/a1/2/features.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps2[['mean_test', 'diff_train_test', 'rstd_test', 'test_over_random', 'test_fold0', 'test_fold1', 'test_fold2', 'test_fold3', 'test_fold4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps3 = gridsearch(pd.read_pickle(f'temp/a1/3/features.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps3[['mean_test', 'diff_train_test', 'rstd_test', 'test_over_random', 'test_fold0', 'test_fold1', 'test_fold2', 'test_fold3', 'test_fold4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps4 = gridsearch(pd.read_pickle(f'temp/a1/4/features.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps4[['mean_test', 'diff_train_test', 'rstd_test', 'test_over_random', 'test_fold0', 'test_fold1', 'test_fold2', 'test_fold3', 'test_fold4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_performance(pd.read_pickle(f'temp/a1/2/features.pkl'), balanced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_performance(pd.read_pickle(f'temp/a1/3/features.pkl'), balanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_performance(pd.read_pickle(f'temp/a1/4/features.pkl'), balanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
